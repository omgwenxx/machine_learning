{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import collections\n",
    "from torch import nn, sigmoid, tanh\n",
    "from scipy.ndimage import gaussian_filter, convolve\n",
    "import time\n",
    "from dataloaders import train_dataloader, test_dataloader\n",
    "from torch import nn, optim\n",
    "from scipy.ndimage import gaussian_filter, convolve\n",
    "from skimage.metrics import mean_squared_error as mse\n",
    "from skimage.metrics import normalized_root_mse as nrmse\n",
    "from skimage.metrics import structural_similarity as ssm\n",
    "from utils import classes, c_to_i, get_orig, show_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = './data/processed/train/'\n",
    "TEST_DIR = './data/processed/test/'\n",
    "\n",
    "classes = os.listdir(TRAIN_DIR)\n",
    "c_to_i = lambda x: classes.index(x)\n",
    "i_to_c = lambda x: classes[x]\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def show_img(im):\n",
    "    plt.imshow(im.reshape(112, 92) / 2 + .5, cmap='gray')\n",
    "    plt.show()\n",
    "    \n",
    "def one_hot(x):\n",
    "    vec = [0] *len(classes)\n",
    "    vec[x] = 1\n",
    "    return vec\n",
    "    \n",
    "train =test= 0\n",
    "train_x, train_y = ([0]) * 280, ([0]) * 280\n",
    "test_x, test_y = ([0]) * 120, ([0]) * 120\n",
    "asd = []\n",
    "\n",
    "for c in os.listdir(TRAIN_DIR):\n",
    "    for faces in os.listdir(TRAIN_DIR+c):\n",
    "        img = np.array(Image.open(TRAIN_DIR+c+'/'+faces).convert('L'))\n",
    "        train_x[train] = (img).flatten()\n",
    "        train_y[train] = (c_to_i(c))\n",
    "        train += 1\n",
    "\n",
    "for c in os.listdir(TEST_DIR):\n",
    "    for faces in os.listdir(TEST_DIR+c):\n",
    "        img = np.array(Image.open(TEST_DIR+c+'/'+faces).convert('L'))\n",
    "        test_x[test] = img.flatten()\n",
    "        test_y[test] = (c_to_i(c))\n",
    "        test += 1\n",
    "\n",
    "train_x = np.stack([x.flatten() for x in train_x])\n",
    "test_x = np.stack([x.flatten() for x in test_x])\n",
    "test_y = np.array(test_y,  dtype=np.int64)\n",
    "train_y = np.array(train_y, dtype=np.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildModel(model, lRate, plot=False, verbose=False):\n",
    "\n",
    "    startTime = time.time()\n",
    "    timeStr = time.strftime(\"%H:%M:%S\", time.localtime(startTime))\n",
    "    print(\"Starting at \" + timeStr + \" to build \" + model.name + \" model...\")\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=lRate)\n",
    "    train_losses, validate_losses, accuracy_data = [], [], []\n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    best_accuracy = 0\n",
    "    iteration_count = 0\n",
    "    total_iteration = 0\n",
    "    \n",
    "    # as in the paper, if there is no improvment after 100 iterations\n",
    "    # stop training\n",
    "    while iteration_count < 100:\n",
    "        total_iteration += 1\n",
    "        if (total_iteration%100==0): print(\"epoch: \" + str(total_iteration))\n",
    "        running_loss = 0\n",
    "        for images, labels in train_dataloader:\n",
    "            output = model(images)\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        # epoch over\n",
    "        iteration_count +=1\n",
    "        \n",
    "        validate_loss = 0\n",
    "        accuracy = 0\n",
    "\n",
    "        # Turn off gradients for validation, saves memory and computations\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for images, labels in test_dataloader:\n",
    "                samples = model(images)\n",
    "                pred = samples.argmax(dim=1)\n",
    "                accuracy = (pred == labels).sum().item() / len(labels)\n",
    "                \n",
    "            model.train()\n",
    "            train_loss = running_loss / len(train_dataloader)\n",
    "            train_losses.append(train_loss)\n",
    "            accuracy_data.append(accuracy)\n",
    "            \n",
    "            \n",
    "            if accuracy > best_accuracy:  # if we improve our accuracy, set the iteration count to 0\n",
    "                if verbose:\n",
    "                    print(\"Epoch: {}.. \".format(total_iteration))\n",
    "                    print('Accuracy increased ({:.2f} --> {:.2f}). Saving model ...'.format(\n",
    "                        best_accuracy, accuracy))\n",
    "                torch.save(model.state_dict(), './models/'+model.name+\"_model.pt\")\n",
    "                iteration_count = 0\n",
    "                best_accuracy = accuracy  # update best accuracy\n",
    "\n",
    "    endTime = time.time()\n",
    "    dur = endTime - startTime\n",
    "    timeStr = time.strftime(\"%H:%M:%S\", time.localtime(endTime))\n",
    "    print(\"Finished at \" + timeStr + \", duration in sec: \" + str(int(dur)))\n",
    "    print(\"Total number of iterations \", total_iteration,\", with accuracy of \", best_accuracy)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at 16:23:17 to build DAE_1000 model...\n"
     ]
    }
   ],
   "source": [
    "# train first dae layer\n",
    "class DAELayer(nn.Module):\n",
    "    def __init__(self, vis, hid):\n",
    "        super(DAELayer, self).__init__()\n",
    "        self.name = f\"DAE_{hid}\"\n",
    "        self.encode = nn.Linear(vis, hid)\n",
    "        self.decode = nn.Linear(hid, vis)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = tanh(self.encode(x))\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "\n",
    "buildModel(DAELayer(10304, 1000), lRate = 1e-4, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
